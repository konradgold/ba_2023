{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-17T20:22:59.919102Z",
     "end_time": "2023-04-17T20:23:01.385534Z"
    }
   },
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "URL = \"https://aclanthology.org/volumes/2022.acl-long/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "ObjectApiResponse({'name': 'es01', 'cluster_name': 'es-docker-single-cluster', 'cluster_uuid': 'T9rYYv05RRWXv_qoiKOQCg', 'version': {'number': '8.5.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'a846182fa16b4ebfcc89aa3c11a11fd5adf3de04', 'build_date': '2022-11-17T18:56:17.538630285Z', 'build_snapshot': False, 'lucene_version': '9.4.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_host = \"http://localhost:9200\"\n",
    "es_user = \"elastic\"\n",
    "es_password = \"1234\"\n",
    "\n",
    "%system docker compose up \"-d\"\n",
    "\n",
    "time.sleep(5.) # give docker time to set up\n",
    "\n",
    "es = elasticsearch.Elasticsearch(\n",
    "    hosts=es_host,  # \"http://localhost:9200\"\n",
    "    basic_auth=(es_user, es_password)   # credentials for basic authentication\n",
    ")\n",
    "\n",
    "time.sleep(5.)\n",
    "\n",
    "es.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T20:23:04.084060Z",
     "end_time": "2023-04-17T20:23:14.797455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "authors_mapping = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"keyword\"},\n",
    "        \"url\": {\"type\": \"keyword\"},\n",
    "        \"papers\": {\"type\": \"keyword\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "setting = {\n",
    "     \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"english_analyzer\": {\n",
    "            \"type\": \"standard\",\n",
    "            \"stopwords\": \"_english_\",\n",
    "            \"tokenizer\": \"standard\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}\n",
    "paper_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\":{\n",
    "        \"properties\": {\n",
    "        \"id\": {\"type\": \"keyword\"},\n",
    "        \"title\": {\"type\": \"text\"},\n",
    "        \"date\": {\"type\": \"integer\"},\n",
    "        \"abstract\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"my_analyzer\"},\n",
    "        \"authors\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# todo: neue Paper?\n",
    "# todo: lieber von Konferenz aus denken - histories erfassen, adativ\n",
    "# todo: crawlerklasse mit spezifischen Unterklassen\n",
    "# todo: holen was geht, stichwort Datapipeline\n",
    "# todo: Embedding abspeichern\n",
    "# todo: Suchfunktion (auf Titel/Abstract) nutze \"swelte\" (Frontend-Programmierung) - Backend: semantische/lexikografische Suche\n",
    "# todo: BA: Analysen (auf Autoren, ...), Topic-Modell - Clustering (Bird-Modeling, sentence-bird) - Frontend mit Topic-Suche\n",
    "# todo: Wissenschaftliche Fragestellung: Verschiedene Topic-Modelle vergleichen, nach Metriken:\n",
    "# todo: Ground-Truths? - Findet man was, z.B. Topics in TOC\n",
    "# todo: dogsfarm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T15:01:19.653690Z",
     "end_time": "2023-04-17T15:01:19.658247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'author'})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#es.options(ignore_status=[400,404]).indices.delete(index='paper')\n",
    "#es.options(ignore_status=[400,404]).indices.delete(index='author')\n",
    "\n",
    "#es.indices.create(index=\"paper\", settings=paper_settings[\"settings\"], mappings=paper_settings[\"mappings\"])\n",
    "#es.indices.create(index=\"author\", mappings=authors_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T15:01:22.899768Z",
     "end_time": "2023-04-17T15:01:23.088628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow open paper  8AuDesp_S3-JJDFIyEJOfA 1 1 142631 0 105.4mb 105.4mb\n",
      "yellow open author rfwpGqw7SCuXzbKyhC18Nw 1 1  69077 0  10.3mb  10.3mb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(es.cat.indices())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T09:30:47.736284Z",
     "end_time": "2023-04-18T09:30:47.856535Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_links_people():\n",
    "    everybody = set()\n",
    "    for letter in tqdm(list(map(chr, range(97, 123)))):\n",
    "        link = \"https://aclanthology.org/people/\" + letter + \"/\"\n",
    "        soup = BeautifulSoup(requests.get(link).content, \"html.parser\")\n",
    "        everybody = everybody.union(set([link + t[\"href\"] for t in soup.find_all(\"a\", attrs={\"href\":re.compile(letter + \".*\")})]))\n",
    "    return everybody"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T20:23:32.033304Z",
     "end_time": "2023-04-17T20:23:32.045436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:58<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "#all_persons = get_links_people()\n",
    "#pickle.dump(all_persons, open(\"remaining_author_links.pkl\", 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T15:01:51.747331Z",
     "end_time": "2023-04-17T15:02:50.061653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_everything_via_author(author_link, host, auth,\n",
    "                              index1, index2, all_ids = set()):\n",
    "    '''\n",
    "    get an authors link + name + papers (id)\n",
    "    get papers id, title, abstract, date + author (via link)\n",
    "    :param author_link:\n",
    "    :return:\n",
    "    '''\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(author_link).content, \"html.parser\")\n",
    "    except:\n",
    "        time.sleep(3.)\n",
    "        soup = BeautifulSoup(requests.get(author_link).content, \"html.parser\")\n",
    "    author_name = soup.h2.text\n",
    "    author_link = author_link.split(\"/\")[-3]+\"/\" + author_link.split(\"/\")[-2]\n",
    "    paper_ids = [id[\"href\"].split(\".bib\")[0].split(\"/\")[1] for id in soup.find_all(\"a\", attrs={\"title\":\"Export to BibTeX\"})]\n",
    "    requests.post(   # make POST request\n",
    "    # set target url -> http://localhost:9200/products/_doc\n",
    "        urljoin(host, f\"{index1}/_doc\"),\n",
    "        auth=(auth[0], auth[1]),\n",
    "        json={\n",
    "            \"name\": author_name,\n",
    "            \"url\": author_link,\n",
    "            \"papers\": paper_ids,\n",
    "        },\n",
    "    )\n",
    "    paper_ids = set(paper_ids).difference(all_ids)\n",
    "    all_ids = all_ids.union(paper_ids)\n",
    "    for p in soup.find_all(lambda tag: tag.name == 'span' and\n",
    "                                   tag.get('class') == ['d-block']):\n",
    "        p_id = p.strong.a[\"href\"].split(\"/\")[1]\n",
    "        if not p_id in paper_ids:\n",
    "            continue\n",
    "        p_title = p.strong.a.text\n",
    "        authors_links = [t[\"href\"].split(\"/\")[-3]+\"/\"+t[\"href\"].split(\"/\")[-2] for t in p.find_all(\"a\", attrs={\"href\":re.compile(r\"/people.*\")})]\n",
    "        abstract_id = \"abstract-\"+ \"--\".join(p_id.split(\".\"))\n",
    "        try:\n",
    "            p_abstract = soup.find(\"div\", attrs={\"id\":f\"{abstract_id}\"}).div.text\n",
    "        except:\n",
    "            p_abstract = None\n",
    "        year = re.findall(r'\\d+', p_id[:4])[0]\n",
    "        if len(year) == 2:\n",
    "            year = int(year)\n",
    "            if year < 45:\n",
    "                year += 2000\n",
    "            else:\n",
    "                year += 1900\n",
    "        else:\n",
    "            year = int(year)\n",
    "\n",
    "        res = requests.post(   # make POST request\n",
    "        # set target url -> http://localhost:9200/products/_doc\n",
    "            urljoin(host, f\"{index2}/_analyze\"),\n",
    "            auth=(auth[0], auth[1]),\n",
    "            json={\n",
    "                \"analyzer\": \"standard\",\n",
    "                \"text\":p_abstract,\n",
    "            },\n",
    "        )\n",
    "        try:\n",
    "            p_abstract = [token[\"token\"] for token in res.json()[\"tokens\"]]\n",
    "        except:\n",
    "            p_abstract=None\n",
    "\n",
    "        requests.post(   # make POST request\n",
    "        # set target url -> http://localhost:9200/products/_doc\n",
    "            urljoin(host, f\"{index2}/_doc\"),\n",
    "            auth=(auth[0], auth[1]),\n",
    "            json={\n",
    "                \"id\": p_id,\n",
    "                \"title\": p_title,\n",
    "                \"date\": year,\n",
    "                \"abstract\": p_abstract,\n",
    "                \"authors\":authors_links\n",
    "            },\n",
    "        )\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def get_index_author(authors_links, host, auth, indices):\n",
    "    all_p_ids = set()\n",
    "    pbar = tqdm(total = len(authors_links)+1)\n",
    "    counter = 0\n",
    "    while authors_links:\n",
    "        link = authors_links.pop()\n",
    "        all_p_ids = get_everything_via_author(link, host, auth, index1=indices[0],\n",
    "                                             index2=indices[1], all_ids=all_p_ids)\n",
    "        if counter >= 10: # Reduce memory accesses\n",
    "            pickle.dump(authors_links, open(\"remaining_author_links.pkl\", 'wb'))\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T20:23:44.403996Z",
     "end_time": "2023-04-17T20:23:44.411872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 42676/42677 [7:25:48<00:00,  1.60it/s]   \n"
     ]
    }
   ],
   "source": [
    "remaining_persons = pickle.load(open(\"./remaining_author_links.pkl\", 'rb'))\n",
    "get_index_author(remaining_persons, host=es_host, auth=[es_user, es_password], indices=[\"author\", \"paper\"])\n",
    "\n",
    "# Expect roughly 1.25it/s"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T20:24:10.269744Z",
     "end_time": "2023-04-18T03:49:58.542258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "es.search(index=\"paper\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "letter = \"a\"\n",
    "soup = BeautifulSoup(requests.get(\"https://aclanthology.org/people/a/\").content, \"html.parser\")\n",
    "[t[\"href\"] for t in soup.find_all(\"a\", attrs={\"href\":re.compile(letter + \".*\")})]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print([t.text for t in soup.find_all(\"a\", attrs={\"href\":re.compile(r\"/people.*\")})])\n",
    "print(soup.find_all(\"div\", attrs={\"class\":\"card bg-light mb-2 mb-lg-3 collapse abstract-collapse\"})[30].div.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.find_all(\"div\", attrs={\"id\":re.compile(r\"abstract-2022--acl-long--.*\")})[0].div.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdapLeR: Speeding up Inference by Adaptive Length Reduction\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(\"a\", attrs={\"class\":\"align-middle\", \"href\":re.compile(r\"/2022.acl-long.[0123456789]*/\")})[1].text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "authors = []\n",
    "for i in range(len(soup.find_all(lambda tag: tag.name == 'span' and\n",
    "                                   tag.get('class') == ['d-block']))):\n",
    "    authors.append([t.text for t in soup.find_all(lambda tag: tag.name == 'span' and\n",
    "                                   tag.get('class') == ['d-block'])[i].find_all(\"a\", attrs={\"href\":re.compile(r\"/people.*\")})])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "for t in soup.find_all(\"div\", attrs={\"id\":re.compile(r\"abstract.*\")}):\n",
    "    abstracts.append(t.div.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "titles = []\n",
    "for i, t in enumerate(soup.find_all(\"a\", attrs={\"class\":\"align-middle\", \"href\":re.compile(r\"/2022.acl-long.[0123456789]*/\")})):\n",
    "    titles.append(t.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "links = []\n",
    "for i in range(len(soup.find_all(lambda tag: tag.name == 'span' and\n",
    "                                   tag.get('class') == ['d-block']))):\n",
    "    links.append([t[\"href\"] for t in soup.find_all(lambda tag: tag.name == 'span' and\n",
    "                                   tag.get('class') == ['d-block'])[i].find_all(\"a\", attrs={\"href\":re.compile(r\"/people.*\")})])\n",
    "all_links = [item for sublist in links for item in sublist]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "authors_with_links = dict(zip([t.text for t in soup.find_all(\"a\", attrs={\"href\":re.compile(r\"/people.*\")})], all_links))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
